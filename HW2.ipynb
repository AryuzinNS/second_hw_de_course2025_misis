{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a9f8a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwx------   - mapred hadoop          0 2025-04-15 18:32 /hadoop\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 18:32 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-16 17:52 /user\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 18:32 /var\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4714c473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bible.txt  HW2.ipynb  mapper.py  reducer.py  Untitled1.ipynb\r\n",
      "hello.txt  map2.py    red2.py    sites.csv\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b1229de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop    4047392 2025-04-16 18:01 /user/files_for_hw/bible.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop   36443383 2025-04-16 18:05 /user/files_for_hw/sites.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/files_for_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f5ea6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bible.txt  hello.txt  HW2.ipynb  sites.csv  Untitled1.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a583dcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-16 18:05 /user/files_for_hw\r\n",
      "drwxr-xr-x   - hive   hadoop          0 2025-04-15 18:32 /user/hive\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-15 18:35 /user/ubuntu\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2fc7a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/files_for_hw/bible.txt': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -put bible.txt /user/files_for_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18eee60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/user/files_for_hw/sites.csv': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -put sites.csv /user/files_for_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df6748fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwx------   - mapred hadoop          0 2025-04-15 18:32 /hadoop\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 18:32 /tmp\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-16 17:52 /user\r\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 18:32 /var\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e83df5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "drwx------   - mapred hadoop          0 2025-04-15 18:32 /hadoop\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 18:32 /tmp\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-16 17:52 /user\n",
      "drwxrwxrwt   - hdfs   hadoop          0 2025-04-15 18:32 /var\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "\n",
    "hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29cb0f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "\n",
    "import sys\n",
    "\n",
    "def mapper():\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                if len(word) > 4:\n",
    "                    print(f\"{word}\\t 1\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1b33326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "import sys\n",
    "\n",
    "def reducer():\n",
    "    current_word = None\n",
    "    current_count = 0\n",
    "    for line in sys.stdin:\n",
    "        word, count = line.strip().split('\\t')\n",
    "        count = int(count)\n",
    "        \n",
    "        if word == current_word:\n",
    "            current_count += count\n",
    "        else:\n",
    "            if current_word is not None:\n",
    "                print(f\"{current_word}\\t{current_count}\")\n",
    "            current_word = word\n",
    "            current_count = count\n",
    "    \n",
    "    if current_word is not None:\n",
    "        print(f\"{current_word}\\t{current_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reducer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c3fece9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall\t9658\n",
      "which\t4119\n",
      "their\t3801\n",
      "there\t1799\n",
      "before\t1701\n",
      "against\t1587\n",
      "shalt\t1586\n",
      "children\t1559\n",
      "said,\t1556\n",
      "them,\t1549\n",
      "LORD,\t1308\n",
      "saying,\t1225\n",
      "house\t1221\n",
      "people\t1194\n",
      "thee,\t1170\n",
      "every\t1150\n",
      "saith\t1135\n",
      "after\t1053\n",
      "Israel\t1025\n",
      "because\t985\n",
      "these\t921\n",
      "among\t894\n",
      "thine\t863\n",
      "great\t835\n",
      "brought\t814\n",
      "things\t780\n",
      "Jesus\t775\n",
      "neither\t762\n",
      "should\t757\n",
      "according\t725\n",
      "Israel,\t723\n",
      "forth\t720\n",
      "David\t687\n",
      "them.\t674\n",
      "bring\t668\n",
      "therefore\t662\n",
      "LORD.\t587\n",
      "behold,\t572\n",
      "heard\t559\n",
      "called\t556\n",
      "people,\t512\n",
      "spake\t508\n",
      "Behold,\t501\n",
      "hundred\t501\n",
      "thee.\t486\n",
      "Moses\t481\n",
      "given\t476\n",
      "heart\t461\n",
      "those\t436\n",
      "about\t431\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## пример запуска скриптов на неймноде для проверки их работы\n",
    "cat bible.txt | python3 mapper.py | sort -k1,1 | python3 reducer.py | sort -k2,2nr | head -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67015be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `/user/files_for_hw/word_count_task': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/files_for_hw/word_count_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a05fc272",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/files_for_hw/url_rank_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b145ac55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   1 ubuntu hadoop    4047392 2025-04-16 18:01 /user/files_for_hw/bible.txt\r\n",
      "-rw-r--r--   1 ubuntu hadoop   36443383 2025-04-16 18:05 /user/files_for_hw/sites.csv\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-18 20:58 /user/files_for_hw/url_rank_task\r\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-04-18 19:50 /user/files_for_hw/word_count_task\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/files_for_hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "012e4f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/files_for_hw/word_count_task\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob4291457022299290327.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 19:49:28,689 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net/10.129.0.28:8032\n",
      "2025-04-18 19:49:28,911 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net/10.129.0.28:10200\n",
      "2025-04-18 19:49:28,947 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net/10.129.0.28:8032\n",
      "2025-04-18 19:49:28,948 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net/10.129.0.28:10200\n",
      "2025-04-18 19:49:29,213 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1745004030788_0007\n",
      "2025-04-18 19:49:30,396 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-18 19:49:30,544 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-18 19:49:30,724 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1745004030788_0007\n",
      "2025-04-18 19:49:30,726 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-18 19:49:30,950 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-18 19:49:30,950 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-18 19:49:31,048 INFO impl.YarnClientImpl: Submitted application application_1745004030788_0007\n",
      "2025-04-18 19:49:31,096 INFO mapreduce.Job: The url to track the job: http://rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net:8088/proxy/application_1745004030788_0007/\n",
      "2025-04-18 19:49:31,097 INFO mapreduce.Job: Running job: job_1745004030788_0007\n",
      "2025-04-18 19:49:38,202 INFO mapreduce.Job: Job job_1745004030788_0007 running in uber mode : false\n",
      "2025-04-18 19:49:38,203 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-18 19:49:45,289 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2025-04-18 19:49:49,314 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2025-04-18 19:49:51,329 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-18 19:49:54,350 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2025-04-18 19:49:57,365 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-04-18 19:49:59,374 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "2025-04-18 19:50:03,401 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2025-04-18 19:50:07,423 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "2025-04-18 19:50:09,432 INFO mapreduce.Job:  map 57% reduce 1%\n",
      "2025-04-18 19:50:15,462 INFO mapreduce.Job:  map 63% reduce 2%\n",
      "2025-04-18 19:50:17,471 INFO mapreduce.Job:  map 67% reduce 2%\n",
      "2025-04-18 19:50:19,480 INFO mapreduce.Job:  map 67% reduce 3%\n",
      "2025-04-18 19:50:20,484 INFO mapreduce.Job:  map 70% reduce 3%\n",
      "2025-04-18 19:50:21,489 INFO mapreduce.Job:  map 73% reduce 4%\n",
      "2025-04-18 19:50:25,511 INFO mapreduce.Job:  map 77% reduce 4%\n",
      "2025-04-18 19:50:26,516 INFO mapreduce.Job:  map 80% reduce 4%\n",
      "2025-04-18 19:50:28,526 INFO mapreduce.Job:  map 83% reduce 4%\n",
      "2025-04-18 19:50:30,536 INFO mapreduce.Job:  map 87% reduce 4%\n",
      "2025-04-18 19:50:31,545 INFO mapreduce.Job:  map 90% reduce 5%\n",
      "2025-04-18 19:50:34,557 INFO mapreduce.Job:  map 93% reduce 5%\n",
      "2025-04-18 19:50:35,562 INFO mapreduce.Job:  map 97% reduce 5%\n",
      "2025-04-18 19:50:36,567 INFO mapreduce.Job:  map 100% reduce 5%\n",
      "2025-04-18 19:50:37,572 INFO mapreduce.Job:  map 100% reduce 11%\n",
      "2025-04-18 19:50:38,577 INFO mapreduce.Job:  map 100% reduce 17%\n",
      "2025-04-18 19:50:39,582 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "2025-04-18 19:50:40,586 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "2025-04-18 19:50:41,591 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "2025-04-18 19:50:45,608 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "2025-04-18 19:50:46,612 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "2025-04-18 19:50:49,623 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-18 19:50:50,634 INFO mapreduce.Job: Job job_1745004030788_0007 completed successfully\n",
      "2025-04-18 19:50:50,715 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=182446\n",
      "\t\tFILE: Number of bytes written=10969154\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7737271\n",
      "\t\tHDFS: Number of bytes written=284955\n",
      "\t\tHDFS: Number of read operations=150\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=36\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled reduce tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=12\n",
      "\t\tData-local map tasks=20\n",
      "\t\tRack-local map tasks=10\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=363930\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=357933\n",
      "\t\tTotal time spent by all map tasks (ms)=121310\n",
      "\t\tTotal time spent by all reduce tasks (ms)=119311\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=121310\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=119311\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=372664320\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=366523392\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=30383\n",
      "\t\tMap output records=266384\n",
      "\t\tMap output bytes=2815321\n",
      "\t\tMap output materialized bytes=587454\n",
      "\t\tInput split bytes=4170\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=26280\n",
      "\t\tReduce shuffle bytes=587454\n",
      "\t\tReduce input records=266384\n",
      "\t\tReduce output records=26280\n",
      "\t\tSpilled Records=532768\n",
      "\t\tShuffled Maps =360\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=360\n",
      "\t\tGC time elapsed (ms)=4523\n",
      "\t\tCPU time spent (ms)=40730\n",
      "\t\tPhysical memory (bytes) snapshot=12299251712\n",
      "\t\tVirtual memory (bytes) snapshot=181908275200\n",
      "\t\tTotal committed heap usage (bytes)=10936123392\n",
      "\t\tPeak Map Physical memory (bytes)=335998976\n",
      "\t\tPeak Map Virtual memory (bytes)=4331511808\n",
      "\t\tPeak Reduce Physical memory (bytes)=249487360\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4336979968\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7733101\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=284955\n",
      "2025-04-18 19:50:50,715 INFO streaming.StreamJob: Output directory: /user/files_for_hw/word_count_task\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## шаблон для запуска MR таски\n",
    "\n",
    "# обязательная чистка директории, куда будем складывать результат отрабоки mr\n",
    "hdfs dfs -rm -r /user/files_for_hw/word_count_task || true\n",
    "\n",
    "# запус mr таски с указанием пути до нужного jar\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"word-count\" \\\n",
    "    -files ./mapper.py,./reducer.py \\\n",
    "    -mapper \"python3 mapper.py\" \\\n",
    "    -reducer \"python3 reducer.py\" \\\n",
    "    -input /user/files_for_hw/bible.txt \\\n",
    "    -output /user/files_for_hw/word_count_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfce2611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lord.\t17\n",
      "pray.\t5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## запустите эту команду, чтобы вывести счетчик определенных слов, которые мы указали на grep\n",
    "## Это нам будет необходимо для визуального анализа результата работы вашего скрипта\n",
    "## в sort можете указать тот разделитель колонок, с которым у вас результат выплевывает редьюсер\n",
    "\n",
    "hdfs dfs -cat /user/files_for_hw/word_count_task/* | grep  -E 'lord\\.|god\\.|pray\\.' | sort -t$'\\t' -k2.2nr  | head -n 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "251784fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting map2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile map2.py\n",
    "\n",
    "import sys\n",
    "\n",
    "def mapper():\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        url, time = line.strip().split(';')\n",
    "        date = time.split(' ')[0]\n",
    "        print(f\"{date}\\t{url}\\t1\")\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a4f76f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting red2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile red2.py\n",
    "import sys\n",
    "\n",
    "def reducer():\n",
    "    result_count = 0\n",
    "    \n",
    "    result_date = None\n",
    "    \n",
    "    result_url = None\n",
    "    \n",
    "\n",
    "    for line in sys.stdin:\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        col = line.strip().split('\\t')\n",
    "        \n",
    "        if len(col) != 3:\n",
    "            continue\n",
    "        \n",
    "        if col[0] == result_date and col[1] == result_url:\n",
    "            result_count += int(col[2])\n",
    "        \n",
    "        else:\n",
    "            if result_date:\n",
    "                print(f\"{result_date}\\t{result_url}\\t{result_count}\")\n",
    "            \n",
    "            result_date = col[0]\n",
    "            result_url = col[1]\n",
    "            result_count = int(col[2])\n",
    "\n",
    "    # Выводим последнюю запись\n",
    "    if result_date:\n",
    "        print(f\"{result_date}\\t{result_url}\\t{result_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    reducer()              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94ab78a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-26\thttps://gonzales-bautista.com/\t335\n",
      "2024-05-26\thttp://smith.com/\t235\n",
      "2024-05-26\thttps://www.smith.com/\t221\n",
      "2024-05-26\thttps://smith.com/\t212\n",
      "2024-05-26\thttp://www.smith.com/\t212\n",
      "2024-05-27\thttps://gonzales-bautista.com/\t376\n",
      "2024-05-27\thttps://www.smith.com/\t270\n",
      "2024-05-27\thttps://smith.com/\t236\n",
      "2024-05-27\thttp://smith.com/\t215\n",
      "2024-05-27\thttp://www.smith.com/\t208\n",
      "2024-05-28\thttps://gonzales-bautista.com/\t368\n",
      "2024-05-28\thttps://smith.com/\t256\n",
      "2024-05-28\thttps://www.smith.com/\t251\n",
      "2024-05-28\thttp://smith.com/\t224\n",
      "2024-05-28\thttp://www.smith.com/\t204\n",
      "2024-05-29\thttps://gonzales-bautista.com/\t402\n",
      "2024-05-29\thttps://www.smith.com/\t242\n",
      "2024-05-29\thttp://www.smith.com/\t223\n",
      "2024-05-29\thttps://smith.com/\t220\n",
      "2024-05-29\thttp://smith.com/\t206\n",
      "2024-05-30\thttps://gonzales-bautista.com/\t353\n",
      "2024-05-30\thttps://smith.com/\t246\n",
      "2024-05-30\thttps://www.smith.com/\t239\n",
      "2024-05-30\thttp://smith.com/\t229\n",
      "2024-05-30\thttp://www.smith.com/\t225\n",
      "2024-05-31\thttps://gonzales-bautista.com/\t374\n",
      "2024-05-31\thttps://www.smith.com/\t244\n",
      "2024-05-31\thttp://smith.com/\t228\n",
      "2024-05-31\thttp://www.smith.com/\t221\n",
      "2024-05-31\thttps://smith.com/\t219\n",
      "2024-06-01\thttps://gonzales-bautista.com/\t379\n",
      "2024-06-01\thttps://www.smith.com/\t232\n",
      "2024-06-01\thttps://smith.com/\t226\n",
      "2024-06-01\thttps://johnson.com/\t195\n",
      "2024-06-01\thttp://smith.com/\t191\n",
      "2024-06-02\thttps://gonzales-bautista.com/\t7\n",
      "2024-06-02\thttp://smith.com/\t7\n",
      "2024-06-02\thttps://www.williams.com/\t6\n",
      "2024-06-02\thttp://lee.com/\t5\n",
      "2024-06-02\thttp://miller.com/\t5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat sites.csv | python3 map2.py | sort -k1,1 | python3 red2.py | sort -k1,1 -k3,3nr | awk '$1 != prev {prev=$1; cnt=1} cnt <=5 {print; cnt++}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c89597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/files_for_hw/url_rank_task\n",
      "packageJobJar: [] [/usr/lib/hadoop-mapreduce/hadoop-streaming-3.2.2.jar] /tmp/streamjob5887909444261507697.jar tmpDir=null\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 21:08:11,968 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net/10.129.0.28:8032\n",
      "2025-04-18 21:08:12,195 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net/10.129.0.28:10200\n",
      "2025-04-18 21:08:12,237 INFO client.RMProxy: Connecting to ResourceManager at rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net/10.129.0.28:8032\n",
      "2025-04-18 21:08:12,238 INFO client.AHSProxy: Connecting to Application History server at rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net/10.129.0.28:10200\n",
      "2025-04-18 21:08:12,490 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/ubuntu/.staging/job_1745004030788_0011\n",
      "2025-04-18 21:08:13,233 INFO mapred.FileInputFormat: Total input files to process : 1\n",
      "2025-04-18 21:08:13,319 INFO mapreduce.JobSubmitter: number of splits:30\n",
      "2025-04-18 21:08:13,496 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1745004030788_0011\n",
      "2025-04-18 21:08:13,497 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-04-18 21:08:13,680 INFO conf.Configuration: resource-types.xml not found\n",
      "2025-04-18 21:08:13,681 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "2025-04-18 21:08:13,751 INFO impl.YarnClientImpl: Submitted application application_1745004030788_0011\n",
      "2025-04-18 21:08:13,782 INFO mapreduce.Job: The url to track the job: http://rc1b-dataproc-m-ifco7qks15ipul9r.mdb.yandexcloud.net:8088/proxy/application_1745004030788_0011/\n",
      "2025-04-18 21:08:13,783 INFO mapreduce.Job: Running job: job_1745004030788_0011\n",
      "2025-04-18 21:08:20,008 INFO mapreduce.Job: Job job_1745004030788_0011 running in uber mode : false\n",
      "2025-04-18 21:08:20,009 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-04-18 21:08:26,075 INFO mapreduce.Job:  map 7% reduce 0%\n",
      "2025-04-18 21:08:28,090 INFO mapreduce.Job:  map 10% reduce 0%\n",
      "2025-04-18 21:08:29,096 INFO mapreduce.Job:  map 13% reduce 0%\n",
      "2025-04-18 21:08:30,103 INFO mapreduce.Job:  map 17% reduce 0%\n",
      "2025-04-18 21:08:32,114 INFO mapreduce.Job:  map 20% reduce 0%\n",
      "2025-04-18 21:08:33,119 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "2025-04-18 21:08:35,130 INFO mapreduce.Job:  map 27% reduce 0%\n",
      "2025-04-18 21:08:37,141 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "2025-04-18 21:08:39,152 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "2025-04-18 21:08:41,164 INFO mapreduce.Job:  map 37% reduce 0%\n",
      "2025-04-18 21:08:44,180 INFO mapreduce.Job:  map 40% reduce 0%\n",
      "2025-04-18 21:08:45,185 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "2025-04-18 21:08:48,202 INFO mapreduce.Job:  map 47% reduce 0%\n",
      "2025-04-18 21:08:49,207 INFO mapreduce.Job:  map 50% reduce 14%\n",
      "2025-04-18 21:08:51,218 INFO mapreduce.Job:  map 53% reduce 14%\n",
      "2025-04-18 21:08:53,227 INFO mapreduce.Job:  map 60% reduce 14%\n",
      "2025-04-18 21:08:55,237 INFO mapreduce.Job:  map 63% reduce 20%\n",
      "2025-04-18 21:08:57,249 INFO mapreduce.Job:  map 67% reduce 20%\n",
      "2025-04-18 21:09:00,264 INFO mapreduce.Job:  map 70% reduce 20%\n",
      "2025-04-18 21:09:01,268 INFO mapreduce.Job:  map 77% reduce 22%\n",
      "2025-04-18 21:09:04,282 INFO mapreduce.Job:  map 80% reduce 22%\n",
      "2025-04-18 21:09:05,288 INFO mapreduce.Job:  map 83% reduce 22%\n",
      "2025-04-18 21:09:07,298 INFO mapreduce.Job:  map 83% reduce 28%\n",
      "2025-04-18 21:09:08,304 INFO mapreduce.Job:  map 87% reduce 28%\n",
      "2025-04-18 21:09:09,310 INFO mapreduce.Job:  map 93% reduce 28%\n",
      "2025-04-18 21:09:10,315 INFO mapreduce.Job:  map 97% reduce 28%\n",
      "2025-04-18 21:09:13,329 INFO mapreduce.Job:  map 100% reduce 32%\n",
      "2025-04-18 21:09:17,347 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "2025-04-18 21:09:18,360 INFO mapreduce.Job: Job job_1745004030788_0011 completed successfully\n",
      "2025-04-18 21:09:18,451 INFO mapreduce.Job: Counters: 56\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=4378427\n",
      "\t\tFILE: Number of bytes written=16374374\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=39222272\n",
      "\t\tHDFS: Number of bytes written=25938557\n",
      "\t\tHDFS: Number of read operations=95\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=3\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\tJob Counters \n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=30\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=12\n",
      "\t\tRack-local map tasks=18\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=287544\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=128820\n",
      "\t\tTotal time spent by all map tasks (ms)=95848\n",
      "\t\tTotal time spent by all reduce tasks (ms)=42940\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=95848\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=42940\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=294445056\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=131911680\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=700000\n",
      "\t\tMap output records=700000\n",
      "\t\tMap output bytes=25943383\n",
      "\t\tMap output materialized bytes=4474049\n",
      "\t\tInput split bytes=4170\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=8\n",
      "\t\tReduce shuffle bytes=4474049\n",
      "\t\tReduce input records=700000\n",
      "\t\tReduce output records=699867\n",
      "\t\tSpilled Records=1400000\n",
      "\t\tShuffled Maps =30\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=30\n",
      "\t\tGC time elapsed (ms)=2191\n",
      "\t\tCPU time spent (ms)=37560\n",
      "\t\tPhysical memory (bytes) snapshot=9660706816\n",
      "\t\tVirtual memory (bytes) snapshot=134242328576\n",
      "\t\tTotal committed heap usage (bytes)=8932294656\n",
      "\t\tPeak Map Physical memory (bytes)=336535552\n",
      "\t\tPeak Map Virtual memory (bytes)=4331585536\n",
      "\t\tPeak Reduce Physical memory (bytes)=294699008\n",
      "\t\tPeak Reduce Virtual memory (bytes)=4333453312\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=39218102\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=25938557\n",
      "2025-04-18 21:09:18,451 INFO streaming.StreamJob: Output directory: /user/files_for_hw/url_rank_task\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## шаблон для запуска MR таски\n",
    "\n",
    "# обязательная чистка директории, куда будем складывать результат отрабоки mr\n",
    "hdfs dfs -rm -r /user/files_for_hw/url_rank_task || true\n",
    "\n",
    "# запус mr таски с указанием пути до нужного jar\n",
    "hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \\\n",
    "    -D mapreduce.job.name=\"url-rank\" \\\n",
    "    -D mapreduce.job.reduces=1 \\\n",
    "    -files ./map2.py,./red2.py \\\n",
    "    -mapper \"python3 map2.py\" \\\n",
    "    -reducer \"python3 red2.py\" \\\n",
    "    -input /user/files_for_hw/sites.csv \\\n",
    "    -output /user/files_for_hw/url_rank_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "324cd69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## запустите эту команду, чтобы вывести результат работы по определенным компаниям, которые мы указали на grep\n",
    "## Это нам будет необходимо для визуального анализа результата работы вашего скрипта\n",
    "## в sort можете указать тот разделитель колонок, с которым у вас результат выплевывает редьюсер\n",
    "## укажите путь до той директории на hdfs, куда вы складывали результат\n",
    "\n",
    "hdfs dfs -cat /user/files_for_hw/url_rank_task/* | grep -E '2024-05-28|2024-06-02|2024-05-30' | column -t -s$'\\t' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d17ded1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
